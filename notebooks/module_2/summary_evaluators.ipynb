{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Summary Evaluators\n",
                "\n",
                "## What I Learned\n",
                "This focused on summary evaluators that calculate metrics like precision, recall, and F1-score across whole experiments rather than individual runs. These are helpful for aggregate performance assessment.\n",
                "\n",
                "## Changes in Code\n",
                "In the notebook, I used summary evaluators on my dataset to get more comprehensive statistics about model output quality and understood how these metrics appear in the LangSmith UI. This completed my understanding of different evaluation levels."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from langsmith import Client\n",
                "from langsmith.evaluation import evaluate\n",
                "from langchain_openai import ChatOpenAI\n",
                "import numpy as np\n",
                "\n",
                "client = Client()\n",
                "\n",
                "def qa_system(inputs: dict) -> dict:\n",
                "    llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
                "    return {\"answer\": llm.invoke(inputs[\"question\"]).content}\n",
                "\n",
                "# Summary evaluator - calculates aggregate metrics\n",
                "def summary_statistics(runs, examples):\n",
                "    \"\"\"Calculate aggregate statistics across all runs\"\"\"\n",
                "    scores = []\n",
                "    \n",
                "    for run in runs:\n",
                "        # Get individual score (example: word overlap)\n",
                "        predicted = run.outputs.get(\"answer\", \"\")\n",
                "        # Calculate a simple metric\n",
                "        score = len(predicted.split()) / 100  # Normalize by length\n",
                "        scores.append(score)\n",
                "    \n",
                "    return {\n",
                "        \"mean_score\": np.mean(scores),\n",
                "        \"median_score\": np.median(scores),\n",
                "        \"std_score\": np.std(scores),\n",
                "        \"min_score\": np.min(scores),\n",
                "        \"max_score\": np.max(scores)\n",
                "    }\n",
                "\n",
                "# Individual evaluator\n",
                "def length_evaluator(run, example):\n",
                "    predicted = run.outputs.get(\"answer\", \"\")\n",
                "    word_count = len(predicted.split())\n",
                "    \n",
                "    # Score based on whether answer is reasonable length (10-100 words)\n",
                "    if 10 <= word_count <= 100:\n",
                "        score = 1.0\n",
                "    else:\n",
                "        score = 0.5\n",
                "    \n",
                "    return {\"key\": \"length_appropriate\", \"score\": score}\n",
                "\n",
                "# Run evaluation with summary evaluator\n",
                "results = evaluate(\n",
                "    qa_system,\n",
                "    data=\"qa_examples\",\n",
                "    evaluators=[length_evaluator],\n",
                "    summary_evaluators=[summary_statistics]\n",
                ")\n",
                "\n",
                "print(f\"Experiment with summary stats: {results['experiment_name']}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.11.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Experiments\n",
                "\n",
                "## What I Learned\n",
                "The session covered how to set up experiments to run evaluations across datasets with control over things like concurrency, repetitions, and dataset splits. It also showed how experiments track metadata and handle different dataset versions.\n",
                "\n",
                "## Changes in Code\n",
                "I ran several experiments on handpicked splits of my dataset, including repeated runs to check result consistency. I also practiced managing metadata and comparing results across different versions."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from langsmith import Client\n",
                "from langsmith.evaluation import evaluate\n",
                "from langchain_openai import ChatOpenAI\n",
                "\n",
                "client = Client()\n",
                "\n",
                "# Define the target function to test\n",
                "def qa_system(inputs: dict) -> dict:\n",
                "    \"\"\"Simple Q&A system using LLM\"\"\"\n",
                "    llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
                "    question = inputs[\"question\"]\n",
                "    response = llm.invoke(question)\n",
                "    return {\"answer\": response.content}\n",
                "\n",
                "# Define evaluator\n",
                "def accuracy_evaluator(run, example):\n",
                "    predicted = run.outputs.get(\"answer\", \"\")\n",
                "    expected = example.outputs.get(\"answer\", \"\")\n",
                "    \n",
                "    # Calculate similarity (simple word overlap)\n",
                "    pred_words = set(predicted.lower().split())\n",
                "    exp_words = set(expected.lower().split())\n",
                "    \n",
                "    if not exp_words:\n",
                "        score = 0.0\n",
                "    else:\n",
                "        overlap = len(pred_words & exp_words)\n",
                "        score = overlap / len(exp_words)\n",
                "    \n",
                "    return {\"key\": \"word_overlap\", \"score\": score}\n",
                "\n",
                "# Run experiment\n",
                "dataset_name = \"qa_examples\"\n",
                "results = evaluate(\n",
                "    qa_system,\n",
                "    data=dataset_name,\n",
                "    evaluators=[accuracy_evaluator],\n",
                "    experiment_prefix=\"qa_experiment\",\n",
                "    metadata={\n",
                "        \"model\": \"gpt-4o-mini\",\n",
                "        \"version\": \"1.0\"\n",
                "    }\n",
                ")\n",
                "\n",
                "print(f\"Experiment completed: {results['experiment_name']}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.11.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
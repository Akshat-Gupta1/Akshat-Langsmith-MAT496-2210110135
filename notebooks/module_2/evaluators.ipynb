{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Evaluators\n",
                "\n",
                "## What I Learned\n",
                "This video explained how evaluators work by scoring model runs against reference outputs based on metrics like correctness or similarity. Evaluators can be defined in code or the LangSmith UI, including using LLMs as judges or custom Python evaluators.\n",
                "\n",
                "## Changes in Code\n",
                "I implemented a similarity-based evaluator for my Q&A dataset, testing how well model responses matched the expected answers using a custom LLM prompt. The code was updated to include these evaluators and run them on the dataset."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from langsmith import Client\n",
                "from langsmith.evaluation import evaluate\n",
                "from langchain_openai import ChatOpenAI\n",
                "\n",
                "client = Client()\n",
                "\n",
                "# Define a custom evaluator\n",
                "def correctness_evaluator(run, example):\n",
                "    \"\"\"Evaluate if the output matches the expected answer\"\"\"\n",
                "    predicted = run.outputs.get(\"answer\", \"\")\n",
                "    expected = example.outputs.get(\"answer\", \"\")\n",
                "    \n",
                "    # Simple string matching\n",
                "    score = 1.0 if predicted.lower() == expected.lower() else 0.0\n",
                "    \n",
                "    return {\n",
                "        \"key\": \"correctness\",\n",
                "        \"score\": score\n",
                "    }\n",
                "\n",
                "# LLM-as-a-judge evaluator\n",
                "def llm_judge_evaluator(run, example):\n",
                "    \"\"\"Use an LLM to judge response quality\"\"\"\n",
                "    llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
                "    \n",
                "    predicted = run.outputs.get(\"answer\", \"\")\n",
                "    expected = example.outputs.get(\"answer\", \"\")\n",
                "    \n",
                "    prompt = f\"\"\"Rate the similarity between these two answers on a scale of 0-1:\n",
                "    \n",
                "Expected: {expected}\n",
                "Predicted: {predicted}\n",
                "\n",
                "Return only a number between 0 and 1.\"\"\"\n",
                "    \n",
                "    response = llm.invoke(prompt)\n",
                "    score = float(response.content.strip())\n",
                "    \n",
                "    return {\n",
                "        \"key\": \"llm_similarity\",\n",
                "        \"score\": score\n",
                "    }\n",
                "\n",
                "print(\"Evaluators defined successfully\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.11.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
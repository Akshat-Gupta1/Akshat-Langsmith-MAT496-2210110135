{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Pairwise Experiments\n",
                "\n",
                "## What I Learned\n",
                "This video introduced pairwise evaluations for comparing two or more experiments side-by-side using custom evaluators and LLM judges. It demonstrated how to set up a pairwise experiment to contrast different prompt versions or model behaviors.\n",
                "\n",
                "## Changes in Code\n",
                "I created a themed pairwise experiment comparing two different approaches to Q&A. I defined an LLM-as-judge evaluator to score the quality of responses and ran the comparison using LangSmith."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from langsmith import Client\n",
                "from langsmith.evaluation import evaluate\n",
                "from langchain_openai import ChatOpenAI\n",
                "\n",
                "client = Client()\n",
                "\n",
                "# Version 1: Simple prompting\n",
                "def qa_simple(inputs: dict) -> dict:\n",
                "    llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
                "    question = inputs[\"question\"]\n",
                "    response = llm.invoke(question)\n",
                "    return {\"answer\": response.content}\n",
                "\n",
                "# Version 2: Detailed prompting\n",
                "def qa_detailed(inputs: dict) -> dict:\n",
                "    llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
                "    question = inputs[\"question\"]\n",
                "    prompt = f\"\"\"Please provide a comprehensive and accurate answer to the following question:\n",
                "    \n",
                "Question: {question}\n",
                "\n",
                "Answer:\"\"\"\n",
                "    response = llm.invoke(prompt)\n",
                "    return {\"answer\": response.content}\n",
                "\n",
                "# Pairwise evaluator\n",
                "def compare_answers(run, example):\n",
                "    \"\"\"Compare two answers using LLM judge\"\"\"\n",
                "    llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
                "    \n",
                "    predicted = run.outputs.get(\"answer\", \"\")\n",
                "    expected = example.outputs.get(\"answer\", \"\")\n",
                "    \n",
                "    judge_prompt = f\"\"\"Compare these two answers and rate the predicted answer's quality (0-1):\n",
                "\n",
                "Expected: {expected}\n",
                "Predicted: {predicted}\n",
                "\n",
                "Return only a number.\"\"\"\n",
                "    \n",
                "    score = float(llm.invoke(judge_prompt).content.strip())\n",
                "    return {\"key\": \"pairwise_quality\", \"score\": score}\n",
                "\n",
                "# Run both experiments\n",
                "print(\"Running pairwise comparison...\")\n",
                "results_simple = evaluate(qa_simple, data=\"qa_examples\", evaluators=[compare_answers])\n",
                "results_detailed = evaluate(qa_detailed, data=\"qa_examples\", evaluators=[compare_answers])\n",
                "\n",
                "print(f\"Simple: {results_simple['experiment_name']}\")\n",
                "print(f\"Detailed: {results_detailed['experiment_name']}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.11.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
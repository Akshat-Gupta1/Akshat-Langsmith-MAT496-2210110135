# Langsmith Akshat Gupta 2210110135 MAT496

This repository documents my learning journey through LangSmith for testing, evaluation, and monitoring of LLM applications.

## Overview

LangSmith is a platform for debugging, testing, evaluating, and monitoring LLM applications. This repository contains notebooks demonstrating key concepts across multiple modules.

## Repository Structure

```
Akshat-Langsmith-MAT496-2210110135/
├── notebooks/
│   ├── module_1/    # Tracing fundamentals
│   │   ├── tracing_basics.ipynb
│   │   ├── types_of_runs.ipynb
│   │   ├── alternative_tracing_methods.ipynb
│   │   ├── conversational_threads.ipynb
│   │   └── utils.py
│   └── module_2/    # Testing and evaluation
│       ├── dataset_upload.ipynb
│       ├── evaluators.ipynb
│       ├── experiments.ipynb
│       ├── pairwise_experiments.ipynb
│       └── summary_evaluators.ipynb
├── requirements.txt # Project dependencies
├── LICENSE          # MIT License
└── README.md        # This file
```

## Modules

### Module 1: Tracing

**Lesson 1: Tracing Basics**  
Learned how to use LangSmith to trace function execution using the `@traceable` decorator from the LangSmith Python SDK. Every function call gets logged as its own run, making debugging and error tracking easier.

**Lesson 2: Types of Runs**  
Explored how LangSmith tracks different types of runs (LLM, retriever, tool, chain). Specifying run types makes tracing clearer and more organized.

**Lesson 3: Alternative Tracing Methods**  
Covered alternative ways to set up tracing, including automatic tracing via environment variables, context managers, and the RunTree API for manual control.

**Lesson 4: Conversational Threads**  
Learned how to group related traces into threads using unique thread IDs to track multi-turn conversations and maintain context.

### Module 2: Testing and Evaluation

**Lesson 1: Datasets**  
Learned how to create and manage datasets in LangSmith using the SDK. Datasets can be added programmatically, manually, or generated by AI for consistent testing.

**Lesson 2: Evaluators**  
Explored how evaluators score model runs against reference outputs using metrics like correctness or similarity. Evaluators can be custom Python functions or LLM-as-judge evaluators.

**Lesson 3: Experiments**  
Learned to set up experiments to run evaluations across datasets with control over concurrency, repetitions, and dataset splits. Experiments track metadata and compare different versions.

**Lesson 4: Pairwise Experiments**  
Discovered how to compare two or more experiments side-by-side using custom evaluators and LLM judges to contrast different prompt versions or model behaviors.

**Lesson 5: Summary Evaluators**  
Learned about summary evaluators that calculate aggregate metrics (precision, recall, F1-score) across entire experiments rather than individual runs for comprehensive performance assessment.

## Getting Started

1. **Install dependencies:**
   ```bash
   pip install -r requirements.txt
   ```

2. **Set up environment variables:**
   ```bash
   export LANGCHAIN_API_KEY="your-api-key"
   export LANGCHAIN_TRACING_V2=true
   export LANGCHAIN_PROJECT="your-project-name"
   ```

3. **Explore the notebooks in order:**
   - Start with Module 1 for tracing fundamentals
   - Move to Module 2 for testing and evaluation

## Key Concepts Learned

- **Tracing**: Logging and visualizing LLM application execution
- **Run Types**: Organizing traces by type (LLM, retriever, tool, chain)
- **Datasets**: Creating consistent test sets for evaluation
- **Evaluators**: Automated scoring of model outputs
- **Experiments**: Running evaluations at scale with metadata tracking
- **Pairwise Comparison**: Comparing different approaches side-by-side
- **Aggregate Metrics**: Understanding overall performance with summary statistics

## Author

**Name**: Akshat Gupta  
**Roll Number**: 2210110135  
**Course**: MAT496  
**Repository**: https://github.com/Akshat-Gupta1/Akshat-Langsmith-MAT496-2210110135

## License

MIT License - see [LICENSE](LICENSE) file for details
